## 1
# voir si la page existe sur wikipÃ©dia
# y'a une API ! et gratuite
# -> wikitextparser
## 2
# il faut prendre un autre lien, mais faut qu'il soit valide ! 
# donc test http code = 200
## 3
# compter combien de liens on a pour philosophy
## 4
# afficher le nombre de liens qu'on a parcouru !

"""
- Ã‰crivez un script qui prend en argument de ligne de commande un nom de page WikipÃ©dia et donne 
  le nombre de sauts nÃ©cessaire pour arriver Ã  la page
  Philosophy ou une erreur si la page en question n'existe pas.
  - Utilisez l'API de WikipÃ©dia pour obtenir le contenu des pages.
  - Vous pouvez parser le wikitexte Ã  la main ou utiliser wikitextparser.
- Si vous Ãªtes trÃ¨s dÃ©terminÃ©â‹…es, faites un script qui prend en entrÃ©e des pages de WikipÃ©dia et
  produit le graphe (orientÃ©) des pages obtenues en suivant Ã  chaque fois le premier lien de chaque
  page, et ce jusqu'Ã  retomber sur une page dÃ©jÃ  visitÃ©e. On pourra par exemple utiliser
  NetworkX, un visualiseur interactif comme pyvis, un wrapper de graphviz ou encore gÃ©nÃ©rer directement 
  des fichiers dot.
"""

import requests
import wikitextparser as wtp
import time

WIKI_API_URL = "https://en.wikipedia.org/w/api.php"
PHILOSOPHY = "Philosophy"

def get_pages_revisions(title):
    """ VÃ©rifie si la page existe et rÃ©cupÃ¨re son le titre et le contenu """

    params = {
        "action": "query",
        "format": "json",
        "prop": "revisions",
        "titles": title,
        "rvprop": "content",
        "rvslots": "main"
    }
    requests_session = requests.Session()
    response = requests_session.get(WIKI_API_URL, params=params)
    data = response.json()
    
    page = next(iter(data["query"]["pages"].values()))
    if "missing" in page:
        print(f"âŒ La page '{title}' n'existe pas.")
        return None
    
    return page["revisions"][0]["slots"]["main"]["*"]

def nettoyage_scraping(wikitext):
    """ Nettoie le wikitext pour retirer les footnotes et images """

    parsed = wtp.parse(wikitext)

    # on enlÃ¨ve les gros paragraphes (footnotes) avant le vrai corps de la page
    text_blocks = [section.string for section in parsed.sections if section.string.strip()]

    if not text_blocks:
        return None 

    return text_blocks[0] 

def extraction_premier_lien(wikitext):
    """ Extrait le premier lien valide, en Ã©vitant les fichiers et liens spÃ©ciaux """
    
    parsed = wtp.parse(wikitext)
    for link in parsed.wikilinks:
        title = link.title

        # on skip les fichiers et le reste sinon Ã§a bug
        if title.startswith(("File:", "Image:", "Media:", "Special:", "Help:", "Wikipedia:")):
            continue

        return title  

    return None

def check_lien_correct(title):
    """ VÃ©rifie si un lien mÃ¨ne Ã  une page valide (HTTP code = 200) """

    response = requests.get(f"https://en.wikipedia.org/wiki/{title}")
    return response.status_code == 200

def compte_liens_philosophy(start_page):
    """ Compte les liens jusqu'Ã  Philosophy """

    visited = set()
    page_actuelle = start_page
    nombre_page = 0

    while page_actuelle.lower() != PHILOSOPHY.lower():
        if page_actuelle in visited:
            print("ğŸ” On tourne en rond !! On arrÃªte")
            return
        
        print(f"ğŸ”— {nombre_page} : {page_actuelle}")
        visited.add(page_actuelle)

        wikitext = get_pages_revisions(page_actuelle)
        if not wikitext:
            print("âŒ Impossible de rÃ©cupÃ©rer la page...")
            return

        prochain_lien = extraction_premier_lien(wikitext)
        if not prochain_lien:
            print("ğŸš« Aucun lien valide trouvÃ©...")
            return
        
        page_actuelle = prochain_lien.replace("_", " ")
        nombre_page += 1
        time.sleep(0.5)  

    print(f"ğŸ‰ ArrivÃ© Ã  'Philosophy' en {nombre_page} Ã©tapes !")

if __name__ == "__main__":
    # rendre le jeu un peu plus simple
    start_page = input("ğŸ” Entrez le nom de la page WikipÃ©dia : ").replace(" ", "_")
    compte_liens_philosophy(start_page)
